{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3390,"status":"ok","timestamp":1725916045371,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"},"user_tz":-60},"id":"IBOjnThXnLb_","outputId":"9a7b000a-f722-47c7-f31b-af1cfa703bb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting cloudpickle==2.1.0\n","  Downloading cloudpickle-2.1.0-py3-none-any.whl.metadata (6.9 kB)\n","Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n","Installing collected packages: cloudpickle\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","Successfully installed cloudpickle-2.1.0\n"]}],"source":["!pip install cloudpickle==2.1.0 # Try installing a compatible version of cloudpickle|"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":793},"executionInfo":{"elapsed":189821,"status":"error","timestamp":1725915885345,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"},"user_tz":-60},"id":"YkF7HM5ijUTe","outputId":"865cd5d1-4051-44a8-ca18-2aff0edbbae0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [976 kB]\n","Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n","Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Get:10 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n","Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,149 kB]\n","Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,288 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,499 kB]\n","Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,224 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,438 kB]\n","Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,570 kB]\n","Fetched 19.5 MB in 5s (4,076 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","sample_data  spark-2.3.1-bin-hadoop2.7\tspark-2.3.1-bin-hadoop2.7.tgz\n"]},{"ename":"TypeError","evalue":"'bytes' object cannot be interpreted as an integer","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-bb96926b2661\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 17\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#create session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBroadcastPickleRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/accumulators.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msocketserver\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSocketServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloudpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPickleSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 146\u001b[0;31m \u001b[0m_cell_set_template_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_cell_set_template_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         )\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 127\u001b[0;31m         return types.CodeType(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_kwonlyargcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'bytes' object cannot be interpreted as an integer"]}],"source":["!apt-get update\n","!apt-get install openjdk-8-jdk-headless -qq \u003e /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n","!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n","\n","!ls\n","\n","import findspark\n","findspark.init()\n","\n","#create session\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","spark"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"elapsed":11133,"status":"error","timestamp":1725916070771,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"},"user_tz":-60},"id":"ZXtmZ8aDnQdm","outputId":"2a50b285-3335-4343-94ee-952164720163"},"outputs":[{"ename":"TypeError","evalue":"'bytes' object cannot be interpreted as an integer","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-3-bded11a88b8f\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBroadcastPickleRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/accumulators.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msocketserver\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSocketServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloudpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPickleSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 146\u001b[0;31m \u001b[0m_cell_set_template_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_cell_set_template_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         )\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 127\u001b[0;31m         return types.CodeType(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_kwonlyargcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'bytes' object cannot be interpreted as an integer"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","spark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":133},"executionInfo":{"elapsed":50013,"status":"ok","timestamp":1589851887770,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"ozh-AQBvjW5s"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'pyspark'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (3/3), done.\n","/content/pyspark\n"]}],"source":["!git clone https://github.com/nicolasmontano/pyspark.git\n","%cd pyspark/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":5995,"status":"ok","timestamp":1589851989367,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"LH_3B7hCjdQr","outputId":"270ffba5-7590-4166-98e2-72af4868f3ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+------+-----+---+--------+--------+---+\n","|interest_rate|credit|march|may|previous|duration|  y|\n","+-------------+------+-----+---+--------+--------+---+\n","|        1.334|     0|    1|  0|       0|     117| no|\n","|        0.767|     0|    0|  2|       1|     274|yes|\n","|        4.858|     0|    1|  0|       0|     167| no|\n","|         4.12|     0|    0|  0|       0|     686|yes|\n","|        4.856|     0|    1|  0|       0|     157| no|\n","|        0.899|     0|    0|  1|       0|     126| no|\n","|         null|     0| null|  0|    null|      84| no|\n","|        4.858|     0|    1|  0|       0|      17| no|\n","|        4.962|     0|    0|  0|       0|     704|yes|\n","|        4.865|     0|    0|  0|       0|     185| no|\n","|        1.365|     0|    0|  1|       1|     374| no|\n","|        0.773|     0|    0|  0|       0|      91|yes|\n","|        0.714|     0|    0|  2|       1|     169|yes|\n","|        4.864|     0|    0|  0|       0|     249| no|\n","|        4.966|     0|    0|  0|       0|     215| no|\n","|        0.904|     0|    0|  0|       0|     324|yes|\n","|        0.849|     0|    0|  2|       1|     159|yes|\n","|        1.811|     1|    0|  0|       0|     120|yes|\n","|        1.264|     0|    1|  0|       0|     337|yes|\n","|        4.076|     0|    0|  0|       0|     640| no|\n","+-------------+------+-----+---+--------+--------+---+\n","only showing top 20 rows\n","\n"]}],"source":["data=spark.read.format('csv').option('header','true').load('pyspark/Bank_data.csv')\n","data.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIj0gVnMkAy7"},"outputs":[],"source":["from pyspark.sql.functions import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUyrKQ7BkUMd"},"outputs":[],"source":["#create table\n","data.createOrReplaceTempView(\"temp\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xtpu2WcYklqR"},"outputs":[],"source":["# Create pd_temp\n","import pandas as pd\n","import numpy as np\n","pd_temp = pd.DataFrame(np.random.random(10))\n","\n","# Create spark_temp from pd_temp\n","spark_temp = spark.createDataFrame(pd_temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"executionInfo":{"elapsed":1616,"status":"ok","timestamp":1589852533142,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"mZSf_rdDmAi5","outputId":"a26324ff-9f49-4fe1-ef3d-36b4d357cd0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","|                  0|\n","+-------------------+\n","| 0.5077882506986768|\n","| 0.7225198216708753|\n","| 0.8610668973087319|\n","| 0.8192756371744561|\n","| 0.1518583856414627|\n","| 0.1494563873799193|\n","| 0.3521464856559823|\n","| 0.9346032125040152|\n","|0.47726924195780884|\n","| 0.4887723646208856|\n","+-------------------+\n","\n"]}],"source":["spark_temp.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":767,"status":"ok","timestamp":1589852616984,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"eNScWIeJmGqO","outputId":"58e872a1-3ce9-4f68-cd31-acec8769a38f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Table(name='table', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"]}],"source":["print(spark.catalog.listTables())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1bohq-lmYzb"},"outputs":[],"source":["#create column\n","data=data.withColumn('aaa',data.credit/10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":852,"status":"ok","timestamp":1589853007895,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"NzhPyW1QnzlS","outputId":"0c13f414-ebec-4274-e7f6-3a6e2f3d1c8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+------+-----+---+--------+--------+---+---+\n","|interest_rate|credit|march|may|previous|duration|  y|aaa|\n","+-------------+------+-----+---+--------+--------+---+---+\n","|        1.334|     0|    1|  0|       0|     117| no|0.0|\n","|        0.767|     0|    0|  2|       1|     274|yes|0.0|\n","|        4.858|     0|    1|  0|       0|     167| no|0.0|\n","|         4.12|     0|    0|  0|       0|     686|yes|0.0|\n","|        4.856|     0|    1|  0|       0|     157| no|0.0|\n","|        0.899|     0|    0|  1|       0|     126| no|0.0|\n","|         null|     0| null|  0|    null|      84| no|0.0|\n","|        4.858|     0|    1|  0|       0|      17| no|0.0|\n","|        4.962|     0|    0|  0|       0|     704|yes|0.0|\n","|        4.865|     0|    0|  0|       0|     185| no|0.0|\n","|        1.365|     0|    0|  1|       1|     374| no|0.0|\n","|        0.773|     0|    0|  0|       0|      91|yes|0.0|\n","|        0.714|     0|    0|  2|       1|     169|yes|0.0|\n","|        4.864|     0|    0|  0|       0|     249| no|0.0|\n","|        4.966|     0|    0|  0|       0|     215| no|0.0|\n","|        0.904|     0|    0|  0|       0|     324|yes|0.0|\n","|        0.849|     0|    0|  2|       1|     159|yes|0.0|\n","|        1.811|     1|    0|  0|       0|     120|yes|0.1|\n","|        1.264|     0|    1|  0|       0|     337|yes|0.0|\n","|        4.076|     0|    0|  0|       0|     640| no|0.0|\n","+-------------+------+-----+---+--------+--------+---+---+\n","only showing top 20 rows\n","\n"]}],"source":["data.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":966,"status":"ok","timestamp":1589853704779,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"pFs2Qx2sn2Lh","outputId":"696b539e-e123-488c-cfe3-33b157f83209"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Table(name='table', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"]}],"source":["print(spark.catalog.listTables())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"elapsed":931,"status":"error","timestamp":1589853727633,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"nHEq9Zzxqk18","outputId":"aca6000c-ab54-4ee9-ff30-5e2046eb559b"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-19-95a293d335d4\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aaa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredit\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"]}],"source":["flights_onehot.select('org', 'org_idx', 'org_dummy').distinct()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":792,"status":"ok","timestamp":1589854087996,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"V9Da3mm3qqbX","outputId":"1758acb0-37aa-4eb0-cbd7-1342004fd30b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+------+-----+---+--------+--------+---+---+\n","|interest_rate|credit|march|may|previous|duration|  y|aaa|\n","+-------------+------+-----+---+--------+--------+---+---+\n","|        0.767|     0|    0|  2|       1|     274|yes|0.0|\n","|        1.365|     0|    0|  1|       1|     374| no|0.0|\n","|        0.695|     0|    1|  2|       1|     403|yes|0.0|\n","|        1.354|     0|    1|  1|       1|     293|yes|0.0|\n","|        0.683|     0|    1|  3|       1|     328|yes|0.0|\n","|        0.682|     0|    1|  1|       1|     360|yes|0.0|\n","|         1.04|     0|    0|  1|       1|     712|yes|0.0|\n","|        0.706|     0|    0|  1|       1|     206|yes|0.0|\n","|        0.668|     0|    1|  3|       1|     268|yes|0.0|\n","|        1.726|     1|    0|  1|       1|     363|yes|0.1|\n","|        0.977|     0|    0|  2|       1|     252|yes|0.0|\n","|        0.652|     1|    0|  2|       1|     450|yes|0.1|\n","|        1.344|     0|    1|  1|       1|     632| no|0.0|\n","|        0.835|     0|    0|  3|       1|     309|yes|0.0|\n","|        0.644|     0|    0|  1|       1|     330|yes|0.0|\n","|        0.896|     0|    0|  1|       1|     292|yes|0.0|\n","|        0.639|     1|    0|  2|       1|     222|yes|0.1|\n","|         1.05|     0|    0|  1|       1|     207| no|0.0|\n","|        0.771|     0|    0|  2|       1|     323|yes|0.0|\n","|        1.029|     0|    0|  1|       1|     738|yes|0.0|\n","+-------------+------+-----+---+--------+--------+---+---+\n","only showing top 20 rows\n","\n"]}],"source":["# Filter flights by passing a string\n","filt_1 = data.filter(\"duration \u003e 200 and previous==1\")\n","\n","# Filter flights by passing a column of boolean values\n","#filt_1 = data.filter(data.duration \u003e 200)\n","\n","filt_1.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"elapsed":755,"status":"ok","timestamp":1589854317919,"user":{"displayName":"Nicolas Montaño","photoUrl":"","userId":"09446804150664905730"},"user_tz":300},"id":"Zw2e0g4XrtJw","outputId":"b3dfc22f-52ff-45e7-b56e-b7775625e37a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+------+-----+---+--------+--------+---+---+\n","|interest_rate|credit|march|may|previous|duration|  y|aaa|\n","+-------------+------+-----+---+--------+--------+---+---+\n","|        0.767|     0|    0|  2|       1|     274|yes|0.0|\n","|        0.714|     0|    0|  2|       1|     169|yes|0.0|\n","|        0.849|     0|    0|  2|       1|     159|yes|0.0|\n","+-------------+------+-----+---+--------+--------+---+---+\n","only showing top 3 rows\n","\n"]}],"source":["# Define first filter\n","filterA = data.y == 'yes'\n","# Define second filter\n","filterB = data.may \u003e= 2\n","# Filter the data, first by filterA then by filterB\n","selected2 = data.filter(filterA).filter(filterB)\n","selected2.show(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BexOZxROsgn3"},"outputs":[],"source":["#select columns\n","# Select the first set of columns\n","selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n","# Define avg_speed\n","avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n","\n","# Select the correct columns\n","speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n","\n","# Create the same table using a SQL expression\n","speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqpyTaiYsg-X"},"outputs":[],"source":["# Find the shortest flight from PDX in terms of distance\n","flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n","\n","# Find the longest flight from SEA in terms of air time\n","flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UF6-l-470rI-"},"outputs":[],"source":["#JOINs\n","airports = airports.withColumnRenamed(\"faa\", \"dest\")\n","\n","# Join the DataFrames\n","flights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n","\n","# Examine the new DataFrame\n","flights_with_airports.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_0FBL5I2nZ0"},"outputs":[],"source":["# Rename year column\n","planes = planes.withColumnRenamed('year','plane_year')\n","#cast types\n","model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast('integer'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQtEslRE00YB"},"outputs":[],"source":["#ML\n","from pyspark.ml.features import StringIndexer,OneHotEncoder\n","\n","# Create a StringIndexer\n","carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n","\n","# Create a OneHotEncoder\n","carr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")\n","onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n","\n","# Make a VectorAssembler\n","vec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")\n","\n","# Import Pipeline\n","from pyspark.ml import Pipeline\n","# Make the pipeline\n","flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n","\n","# Fit and transform the data\n","piped_data = flights_pipe.fit(model_data).transform(model_data)\n","\n","# Split the data into training and test sets\n","training, test = piped_data.randomSplit([0.8, 0.2], seed=17)\n","\n","# Import the evaluation submodule\n","import pyspark.ml.evaluation as evals\n","\n","# Create a BinaryClassificationEvaluator\n","evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YK0FtMjnJHI9"},"outputs":[],"source":["from pyspark.ml.classification import LogisticRegression\n","# Create a LogisticRegression Estimator\n","lr = LogisticRegression()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfRHKvOyLSjv"},"outputs":[],"source":["# Import the tuning submodule\n","import pyspark.ml.tuning as tune\n","\n","# Create the parameter grid\n","grid = tune.ParamGridBuilder()\n","\n","# Add the hyperparameter\n","grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n","grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n","\n","# Build the grid\n","grid = grid.build()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeIU7L84MHbP"},"outputs":[],"source":["# Create the CrossValidator\n","cv = tune.CrossValidator(estimator=lr,\n","                         estimatorParamMaps=grid,\n","                         evaluator=evaluator\n","                         )\n","# Fit cross validation models\n","models = cv.fit(training)\n","\n","# Extract the best model\n","best_lr = models.bestModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVcChZU6NxLG"},"outputs":[],"source":["# Use the model to predict the test set\n","test_results = best_lr.transform(test)\n","\n","# Evaluate the predictions\n","print(evaluator.evaluate(test_results))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATW-qcHFZ7Ig"},"outputs":[],"source":["# Import the necessary class\n","from pyspark.ml.feature import VectorAssembler\n","\n","# Create an assembler object\n","assembler = VectorAssembler(inputCols=[\n","    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n","], outputCol='features')\n","\n","# Consolidate predictor columns\n","flights_assembled = assembler.transform(flights)\n","flights_assembled.show(3)\n","# Check the resulting column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhG--8pS9Dr_"},"outputs":[],"source":["flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqxwEUGW-38B"},"outputs":[],"source":["#Regression\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# Create a regression object and train on training data\n","regression = LinearRegression(labelCol='duration').fit(flights_train)\n","\n","# Create predictions for the testing data and take a look at the predictions\n","predictions = regression.transform(flights_test)\n","predictions.select('duration', 'prediction').show(5, False)\n","\n","# Calculate the RMSE\n","RegressionEvaluator(labelCol='duration').evaluate(predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wqsvYeP_gba"},"outputs":[],"source":["# Intercept (average minutes on ground)\n","inter = regression.intercept\n","print(inter)\n","\n","# Coefficients\n","coefs = regression.coefficients\n","print(coefs)\n","\n","# Average minutes per km\n","minutes_per_km = regression.coefficients[0]\n","print(minutes_per_km)\n","\n","# Average speed in km per hour\n","avg_speed = 60 / minutes_per_km\n","print(avg_speed)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMMkTQdWC7CTZbjoCFSYIzq","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}